{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning using NLP techniques\n",
    "Documents used for ML decision making need to be pre-processed i.e. cleaned. These steps include:\n",
    "    - Tokenization    \n",
    "    - Lowercasing\n",
    "    - Noise removal \n",
    "    - Removing stop words \n",
    "    - Text normalization\n",
    "    - Stemming\n",
    "    - Lemmatization    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "    - Tokenization is the process of breaking down a corpus into individual tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "    - Converts all the existing uppercase characters into lowercase ones so that the entire corpus is in lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Removal\n",
    "    - Removing noise in the corpus such as puntuations, hashtags, html markup, non-ASCII characters \n",
    "    - and other unwanted characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "    - Removing standard/typical stop words from the corpus (i.e. such as those define in NLTK library)\n",
    "    - Removing user specified domain specific stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "    - This is the process of converting a raw corpus into a canonical and standard form, which is basically to ensure that \n",
    "    the textual input is guaranteed to be consistent before it is analyzed, processed, and operated upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "    - Stemming is performed on a corpus to reduce words to their stem or root form. The reason for saying \"stem or root form\" is that the process of stemming doesn't always reduce the word to its root but sometimes just to its canonical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer\n",
    "    - Lemmatization is a process that is like stemming â€“ its purpose is to reduce a word to its root form. What makes it different is that it doesn't just chop the ends of words off to obtain this root form, but instead follows a process, abides by rules, and often uses WordNet for mappings to return words to their root forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPEngine(object):\n",
    "        \"\"\"\n",
    "        Natural Language Processing engine used to pre-process a corpus\n",
    "        \"\"\"\n",
    "        def __init__(self,docs, is_stemmimg=False, is_lemmentization=False):\n",
    "            self.__docs = docs\n",
    "            self.__is_stemming = is_stemmimg\n",
    "            self.__is_lemmentization = is_lemmentization\n",
    "            self.__stop_words = self.getStopWords()\n",
    "            if self.__is_stemming: \n",
    "                self.__stemmer = PorterStemmer()\n",
    "            else:\n",
    "                self.__stemmer = None\n",
    "            if self.__is_lemmentization: \n",
    "                self.__lemmatizer = WordNetLemmatizer()\n",
    "            else:\n",
    "                self.__lemmatizer = None\n",
    "           \n",
    "            \n",
    "        def removeNoise(self, text):\n",
    "            #remove html markup\n",
    "            text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "            #remove non-ascii and digits\n",
    "            text=re.sub(\"(\\W|\\d+)\",\" \",text)\n",
    "            # prepare regex to filter punctuations\n",
    "            re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "            text=re_punc.sub('', text)\n",
    "            #remove whitespace\n",
    "            text=text.strip()\n",
    "            return text\n",
    "        \n",
    "        def normalize(self,text):\n",
    "            normalize_dict = {\n",
    "                brb: \"be right back\"\n",
    "            }\n",
    "            new_text = normalize_dict.get(text)\n",
    "            if new_text is None:\n",
    "                return text\n",
    "            else:\n",
    "                return new_text\n",
    "            \n",
    "        def getStopWords(self):\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            user_defined_stop_words = []\n",
    "            all_stop_words = []\n",
    "            all_stop_words.extend(stop_words)\n",
    "            all_stop_words.extend(user_defined_stop_words)\n",
    "            all_stop_words = list(set(all_stop_words))\n",
    "            return all_stop_words\n",
    "            \n",
    "            \n",
    "            \n",
    "        def preprocessDoc(self, doc):\n",
    "            # Tokenization\n",
    "            tokens_0 = word_tokenize(doc)\n",
    "            # Lowercasing/uppercasing\n",
    "            tokens_1 = [w.lower() for w in tokens_0]\n",
    "            # remove punctuation from each word\n",
    "            tokens_2 = [self.removeNoise(w) for w in tokens_1]\n",
    "            # remove remaining tokens that are not alphabetic\n",
    "            tokens_3 = [word for word in tokens_2 if word.isalpha()]\n",
    "            # Removing stop words\n",
    "            tokens_4 = [w for w in tokens_3 if not w in self.__stop_words]\n",
    "            if self.__is_stemming:\n",
    "                tokens_5 =[self.__stemmer.stem(word = word) for word in tokens_4]\n",
    "            else:\n",
    "                tokens_5 = tokens_4\n",
    "            if self.__is_lemmentization:\n",
    "                tokens_6 =[self.__lemmatizer.lemmatize(word = word, pos = 'v') for word in tokens_5]\n",
    "            else:\n",
    "                tokens_6 = tokens_5\n",
    "            return tokens_6\n",
    "        \n",
    "        def preprocessDocs(self):\n",
    "            preprocess_docs = [self.preprocessDoc(doc) for doc in self.__docs]\n",
    "            return preprocess_docs\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NLP Preprocessing Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the test data:\n",
    "    - Test data was was used for a Kaggle competition it is the Quora pair dataset Quora dataset [further details here](https://www.kaggle.com/currie32/predicting-similarity-tfidfvectorizer-doc2vec/data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus = pd.read_csv(r\"Data\\questions.csv\")\n",
    "df_sample_corpus = df_corpus[:100]\n",
    "df_sample_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNlpEngine():\n",
    "    docs = df_sample_corpus.question1.tolist()\n",
    "    nlp_engine = NLPEngine(docs)\n",
    "    clean_docs = nlp_engine.preprocessDocs()\n",
    "   \n",
    "    \n",
    "testNlpEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
